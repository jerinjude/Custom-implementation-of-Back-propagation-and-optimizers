# Custom-implementation-of-Back-propagation-and-optimizers

Implemented backpropagation for a computational graph using basic python libraries like NumPy. The partial derivatives of loss function with respect to each weight were validated using approximate derivative method. 
Optimizers like SGD, SGD with momentum and ADAM were also implemented from scratch
Graphs between loss and epochs were plotted to compare each optimizer.
